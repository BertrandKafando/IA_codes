{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BertrandKafando/MachineLearning/blob/master/A_CNN_for_facial_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwjKxMjWqDLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459ca3ed-f1f2-4af9-eb17-600be572e033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7yG5svHmhnI",
        "outputId": "de68095f-8dc3-49de-8e26-d994e6fc79c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 244 images belonging to 16 classes.\n",
            "Found 244 images belonging to 16 classes.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'face1': 0,\n",
              " 'face10': 1,\n",
              " 'face11': 2,\n",
              " 'face12': 3,\n",
              " 'face13': 4,\n",
              " 'face14': 5,\n",
              " 'face15': 6,\n",
              " 'face16': 7,\n",
              " 'face2': 8,\n",
              " 'face3': 9,\n",
              " 'face4': 10,\n",
              " 'face5': 11,\n",
              " 'face6': 12,\n",
              " 'face7': 13,\n",
              " 'face8': 14,\n",
              " 'face9': 15}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Deep Learning CNN model to recognize face\n",
        "'''This script uses a database of images and creates CNN model on top of it to test\n",
        "   if the given image is recognized correctly or not'''\n",
        "\n",
        "'''####### IMAGE PRE-PROCESSING for TRAINING and TESTING data #######'''\n",
        "\n",
        "# Specifying the folder where images are present\n",
        "TrainingImagePath='/content/drive/MyDrive/Face Images/Final Training Images'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# Understand more about ImageDataGenerator at below link\n",
        "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
        "\n",
        "# Defining pre-processing transformations on raw images of training data\n",
        "# These hyper parameters helps to generate slightly twisted versions\n",
        "# of the original image, which leads to a better model, since it learns\n",
        "# on the good and bad mix of images\n",
        "train_datagen = ImageDataGenerator(\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "# Defining pre-processing transformations on raw images of testing data\n",
        "# No transformations are done on the testing images\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "# Generating the Training Data\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        TrainingImagePath,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "\n",
        "\n",
        "# Generating the Testing Data\n",
        "validation_set = test_datagen.flow_from_directory(\n",
        "        TrainingImagePath,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Printing class labels for each face\n",
        "validation_set.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set.image_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o9Jgm87XGvx",
        "outputId": "12b7e127-23be-4b81-ef33-c03f3025eaf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set."
      ],
      "metadata": {
        "id": "pd58h6TmX-YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1qM3wiFsOZj"
      },
      "source": [
        "Creating a mapping for index and face names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKqW3t8osCKK",
        "outputId": "61177f1b-32ec-4ee5-ffbf-5fc97be5f6cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping of Face and its ID {0: 'face1', 1: 'face10', 2: 'face11', 3: 'face12', 4: 'face13', 5: 'face14', 6: 'face15', 7: 'face16', 8: 'face2', 9: 'face3', 10: 'face4', 11: 'face5', 12: 'face6', 13: 'face7', 14: 'face8', 15: 'face9'}\n",
            "\n",
            " The Number of output neurons:  16\n"
          ]
        }
      ],
      "source": [
        "'''############ Creating lookup table for all faces ############'''\n",
        "# class_indices have the numeric tag for each face\n",
        "TrainClasses=training_set.class_indices\n",
        "\n",
        "# Storing the face and the numeric tag for future reference\n",
        "ResultMap={}\n",
        "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
        "    ResultMap[faceValue]=faceName\n",
        "\n",
        "# Saving the face map for future reference\n",
        "import pickle\n",
        "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
        "    pickle.dump(ResultMap, fileWriteStream)\n",
        "\n",
        "# The model will give answer as a numeric tag\n",
        "# This mapping will help to get the corresponding face name for it\n",
        "print(\"Mapping of Face and its ID\",ResultMap)\n",
        "\n",
        "# The number of neurons for the output layer is equal to the number of faces\n",
        "OutputNeurons=len(ResultMap)\n",
        "print('\\n The Number of output neurons: ', OutputNeurons)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the CNN face recognition model\n",
        "In the below code snippet, I have created a CNN model with\n",
        "\n",
        "2 hidden layers of convolution\n",
        "2 hidden layers of max pooling\n",
        "1 layer of flattening\n",
        "1 Hidden ANN layer\n",
        "1 output layer with 16-neurons (one for each face)\n",
        "You can increase or decrease the convolution, max pooling, and hidden ANN layers and the number of neurons in it.\n",
        "\n",
        "Just keep in mind, the more layers/neurons you add, the slower the model becomes."
      ],
      "metadata": {
        "id": "tVHNjaoMtHMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''######################## Create CNN deep learning model ########################'''\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPool2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n",
        "'''Initializing the Convolutional Neural Network'''\n",
        "classifier= Sequential()\n",
        "\n",
        "''' STEP--1 Convolution\n",
        "# Adding the first layer of CNN\n",
        "# we are using the format (64,64,3) because we are using TensorFlow backend\n",
        "# It means 3 matrix of size (64X64) pixels representing Red, Green and Blue components of pixels\n",
        "'''\n",
        "classifier.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=(64,64,3), activation='relu'))\n",
        "\n",
        "'''# STEP--2 MAX Pooling'''\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "'''############## ADDITIONAL LAYER of CONVOLUTION for better accuracy #################'''\n",
        "classifier.add(Convolution2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
        "\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "'''# STEP--3 FLattening'''\n",
        "classifier.add(Flatten())\n",
        "\n",
        "'''# STEP--4 Fully Connected Neural Network'''\n",
        "classifier.add(Dense(64, activation='relu'))\n",
        "\n",
        "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
        "\n",
        "'''# Compiling the CNN'''\n",
        "#classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
        "\n",
        "###########################################################\n",
        "import time\n",
        "# Measuring the time taken by the model to train\n",
        "StartTime=time.time()\n",
        "\n",
        "# Starting the model training\n",
        "classifier.fit_generator(\n",
        "                    training_set,\n",
        "                    steps_per_epoch=len(training_set),\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_set,\n",
        "                    validation_steps=10)\n",
        "\n",
        "EndTime=time.time()\n",
        "print(\"###### Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes ######')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8jQewSbtKxb",
        "outputId": "c9bafe41-785b-4fbe-d743-2739b895326f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-21f59c7b6904>:44: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  classifier.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - ETA: 0s - loss: 77.2418 - accuracy: 0.0820"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8/8 [==============================] - 6s 671ms/step - loss: 77.2418 - accuracy: 0.0820 - val_loss: 3.0094 - val_accuracy: 0.0697\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 3s 406ms/step - loss: 2.6705 - accuracy: 0.1803\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 3s 391ms/step - loss: 1.9684 - accuracy: 0.4057\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 3s 395ms/step - loss: 1.2930 - accuracy: 0.6230\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 3s 406ms/step - loss: 0.5583 - accuracy: 0.8607\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 3s 406ms/step - loss: 0.3125 - accuracy: 0.8770\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 3s 401ms/step - loss: 0.2354 - accuracy: 0.9385\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 3s 400ms/step - loss: 0.2067 - accuracy: 0.9344\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 3s 395ms/step - loss: 0.2065 - accuracy: 0.9590\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 3s 395ms/step - loss: 0.1518 - accuracy: 0.9549\n",
            "###### Total Time Taken:  1 Minutes ######\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''########### Making single predictions ###########'''\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "ImagePath='/content/drive/MyDrive/Face Images/Final Testing Images/face16/3face16.jpg'\n",
        "test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
        "test_image=image.img_to_array(test_image)\n",
        "\n",
        "test_image=np.expand_dims(test_image,axis=0)\n",
        "\n",
        "result=classifier.predict(test_image,verbose=0)\n",
        "#print(training_set.class_indices)\n",
        "\n",
        "print('####'*10)\n",
        "print('Prediction is: ',ResultMap[np.argmax(result)], \"   |   Expected/reality : \", \" face16\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QxuT-WYwUST",
        "outputId": "5ddef480-b1fa-431c-d883-7d8f53c8c88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################################\n",
            "Prediction is:  face16    |   Expected/reality :   face16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''########### Making single predictions ###########'''\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "s = 0\n",
        "v = 0\n",
        "path='/content/drive/MyDrive/Face Images/Final Testing Images'\n",
        "for file in os.listdir(path):\n",
        "  # get image path\n",
        "  if file!=\".DS_Store\":\n",
        "    filePath=os.path.join(path,file)\n",
        "    for im in os.listdir(filePath): \n",
        "      ImagePath=os.path.join(filePath,im)\n",
        "      test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
        "      test_image=image.img_to_array(test_image)\n",
        "      test_image=np.expand_dims(test_image,axis=0)\n",
        "      result=classifier.predict(test_image,verbose=0) \n",
        "      print('####'*10)\n",
        "      print(f'Real:{im[1:]} Prediction: {ResultMap[np.argmax(result)]}')\n",
        "      s+=1\n",
        "      if(im[1:].split(\".\")[0] == ResultMap[np.argmax(result)]):\n",
        "        v+=1\n",
        "\n",
        "print(v)\n",
        "print(s)\n",
        "print(f'Accuracy: {v/s}')"
      ],
      "metadata": {
        "id": "gCXI_Wj0w3nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe59ce5-6f99-4054-e829-02ceb6079f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################################\n",
            "Real:face1.jpg Prediction: face14\n",
            "########################################\n",
            "Real:face1.jpg Prediction: face1\n",
            "########################################\n",
            "Real:face1.jpg Prediction: face1\n",
            "########################################\n",
            "Real:face1.jpg Prediction: face14\n",
            "########################################\n",
            "Real:face14.jpg Prediction: face14\n",
            "########################################\n",
            "Real:face14.jpg Prediction: face14\n",
            "########################################\n",
            "Real:face14.jpg Prediction: face14\n",
            "########################################\n",
            "Real:face14.jpg Prediction: face14\n",
            "########################################\n",
            "Real:face15.jpg Prediction: face15\n",
            "########################################\n",
            "Real:face15.jpg Prediction: face15\n",
            "########################################\n",
            "Real:face15.jpg Prediction: face15\n",
            "########################################\n",
            "Real:face15.jpg Prediction: face15\n",
            "########################################\n",
            "Real:face16.jpg Prediction: face16\n",
            "########################################\n",
            "Real:face16.jpg Prediction: face16\n",
            "########################################\n",
            "Real:face16.jpg Prediction: face16\n",
            "########################################\n",
            "Real:face16.jpg Prediction: face16\n",
            "########################################\n",
            "Real:face11.jpg Prediction: face11\n",
            "########################################\n",
            "Real:face11.jpg Prediction: face11\n",
            "########################################\n",
            "Real:face11.jpg Prediction: face11\n",
            "########################################\n",
            "Real:face11.jpg Prediction: face11\n",
            "########################################\n",
            "Real:face2.jpg Prediction: face2\n",
            "########################################\n",
            "Real:face2.jpg Prediction: face2\n",
            "########################################\n",
            "Real:face2.jpg Prediction: face2\n",
            "########################################\n",
            "Real:face2.jpg Prediction: face2\n",
            "########################################\n",
            "Real:face13.jpg Prediction: face13\n",
            "########################################\n",
            "Real:face13.jpg Prediction: face13\n",
            "########################################\n",
            "Real:face13.jpg Prediction: face13\n",
            "########################################\n",
            "Real:face13.jpg Prediction: face13\n",
            "########################################\n",
            "Real:face3.jpg Prediction: face3\n",
            "########################################\n",
            "Real:face3.jpg Prediction: face3\n",
            "########################################\n",
            "Real:face3.jpg Prediction: face3\n",
            "########################################\n",
            "Real:face3.jpg Prediction: face3\n",
            "########################################\n",
            "Real:face12.jpg Prediction: face12\n",
            "########################################\n",
            "Real:face12.jpg Prediction: face12\n",
            "########################################\n",
            "Real:face12.jpg Prediction: face12\n",
            "########################################\n",
            "Real:face12.jpg Prediction: face12\n",
            "########################################\n",
            "Real:face10.jpg Prediction: face10\n",
            "########################################\n",
            "Real:face10.jpg Prediction: face10\n",
            "########################################\n",
            "Real:face10.jpg Prediction: face10\n",
            "########################################\n",
            "Real:face10.jpg Prediction: face10\n",
            "########################################\n",
            "Real:face9.jpg Prediction: face9\n",
            "########################################\n",
            "Real:face9.jpg Prediction: face9\n",
            "########################################\n",
            "Real:face9.jpg Prediction: face9\n",
            "########################################\n",
            "Real:face9.jpg Prediction: face9\n",
            "########################################\n",
            "Real:face7.jpg Prediction: face7\n",
            "########################################\n",
            "Real:face7.jpg Prediction: face7\n",
            "########################################\n",
            "Real:face7.jpg Prediction: face7\n",
            "########################################\n",
            "Real:face7.jpg Prediction: face7\n",
            "########################################\n",
            "Real:face8.jpg Prediction: face8\n",
            "########################################\n",
            "Real:face8.jpg Prediction: face8\n",
            "########################################\n",
            "Real:face8.jpg Prediction: face8\n",
            "########################################\n",
            "Real:face8.jpg Prediction: face8\n",
            "########################################\n",
            "Real:face5.jpg Prediction: face5\n",
            "########################################\n",
            "Real:face5.jpg Prediction: face5\n",
            "########################################\n",
            "Real:face5.jpg Prediction: face5\n",
            "########################################\n",
            "Real:face5.jpg Prediction: face5\n",
            "########################################\n",
            "Real:face4.jpg Prediction: face4\n",
            "########################################\n",
            "Real:face4.jpg Prediction: face4\n",
            "########################################\n",
            "Real:face4.jpg Prediction: face4\n",
            "########################################\n",
            "Real:face4.jpg Prediction: face4\n",
            "########################################\n",
            "Real:face6.jpg Prediction: face6\n",
            "########################################\n",
            "Real:face6.jpg Prediction: face6\n",
            "########################################\n",
            "Real:face6.jpg Prediction: face6\n",
            "########################################\n",
            "Real:face6.jpg Prediction: face6\n",
            "62\n",
            "64\n",
            "Accuracy: 0.96875\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}